# DevLog 1-3: Node.js & Web Scraping with Cron

## Project Overview
Built a Node.js application that uses Puppeteer to scrape GitHub repository data on a scheduled interval using cron jobs.

## Key Accomplishments

### Environment Setup
- Installed and configured Node.js (v20.9.0)
- Initialized NPM project with ES module support (`"type": "module"`)
- Installed Nodemon globally for automatic script reloading during development

### Dependencies Installed
- **cron** (^4.4.0) - For scheduling automated tasks
- **puppeteer** (^24.36.1) - Headless browser automation for web scraping

### Core Implementation
Created `index.js` with the following functionality:

#### Web Scraping Function
- Launches headless Chromium browser using Puppeteer
- Navigates to GitHub repository: `https://github.com/alpnix/Radical-Software-DevLogs`
- Extracts structured data:
  - Page title
  - Repository name
  - Repository description
  - Star count
  - First 5 files in the directory listing
- Logs timestamped results to console

#### Cron Job Scheduling
- Configured cron expression: `*/10 * * * *` (runs every 10 minutes)
- Automatically executes the scraping function on schedule
- Job starts immediately upon script execution

### Version Control
- Initialized Git repository
- Created `.gitignore` to exclude `node_modules` and other unnecessary files
- Published to GitHub: [alpnix/Radical-Software-DevLogs/1-3](https://github.com/alpnix/Radical-Software-DevLogs/tree/master/1-3)

## Technical Highlights
- Used modern ES6 imports
- Implemented async/await for asynchronous browser operations
- Used DOM query selectors with optional chaining for safe data extraction
- Configured Puppeteer with `networkidle2` for reliable page loading

## Commits
- Initial commit with project structure
- Added scraping functionality
- Updated GitHub link in README

## Reflection Addendum
This project reinforced how cron scheduling and headless browser automation fit together for repeatable data collection. It also clarified the importance of defensive DOM queries when scraping pages that may change over time.
